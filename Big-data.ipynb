{
 "metadata": {
  "name": "",
  "signature": "sha256:4212db9fed79976559a1e07d0ecd280470ab8d0de6173bc78a9e2d6acdcd2149"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some general guidelines I've found over the years.\n",
      "\n",
      "How large is your training set?\n",
      "\n",
      "If your training set is small, high bias/low variance classifiers (e.g., Naive Bayes) have an advantage over low bias/high variance classifiers (e.g., kNN or logistic regression), since the latter will overfit. But low bias/high variance classifiers start to win out as your training set grows (they have lower asymptotic error), since high bias classifiers aren't powerful enough to provide accurate models. \n",
      "\n",
      "You can also think of this as a generative model vs. discriminative model distinction.\n",
      "\n",
      "Advantages of some particular algorithms\n",
      "\n",
      "Advantages of Naive Bayes: Super simple, you're just doing a bunch of counts. If the NB conditional independence assumption actually holds, a Naive Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. And even if the NB assumption doesn't hold, a NB classifier still often performs surprisingly well in practice. A good bet if you want to do some kind of semi-supervised learning, or want something embarrassingly simple that performs pretty well.\n",
      "\n",
      "Advantages of Logistic Regression: Lots of ways to regularize your model, and you don't have to worry as much about your features being correlated, like you do in Naive Bayes. You also have a nice probabilistic interpretation, unlike decision trees or SVMs, and you can easily update your model to take in new data (using an online gradient descent method), again unlike decision trees or SVMs. Use it if you want a probabilistic framework (e.g., to easily adjust classification thresholds, to say when you're unsure, or to get confidence intervals) or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model.\n",
      "\n",
      "Advantages of Decision Trees: Easy to interpret and explain (for some people -- I'm not sure I fall into this camp). Non-parametric, so you don't have to worry about outliers or whether the data is linearly separable (e.g., decision trees easily take care of cases where you have class A at the low end of some feature x, class B in the mid-range of feature x, and A again at the high end). Their main disadvantage is that they easily overfit, but that's where ensemble methods like random forests (or boosted trees) come in. Plus, random forests are often the winner for lots of problems in classification (usually slightly ahead of SVMs, I believe), they're fast and scalable, and you don't have to worry about tuning a bunch of parameters like you do with SVMs, so they seem to be quite popular these days.\n",
      "\n",
      "Advantages of SVMs: High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you're data isn't linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm. Memory-intensive and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.\n",
      "\n",
      "To go back to the particular question of logistic regression vs. decision trees (which I'll assume to be a question of logistic regression vs. random forests) and summarize a bit: both are fast and scalable, random forests tend to beat out logistic regression in terms of accuracy, but logistic regression can be updated online and gives you useful probabilities. And since you're at Square (not quite sure what an inference scientist is, other than the embodiment of fun) and possibly working on fraud detection: having probabilities associated to each classification might be useful if you want to quickly adjust thresholds to change false positive/false negative rates, and regardless of the algorithm you choose, if your classes are heavily imbalanced (as often happens with fraud), you should probably resample the classes or adjust your error metrics to make the classes more equal.\n",
      "\n",
      "But...\n",
      "\n",
      "Recall, though, that better data often beats better algorithms, and designing good features goes a long way. And if you have a huge dataset, your choice of classification algorithm might not really matter so much in terms of classification performance (so choose your algorithm based on speed or ease of use instead).\n",
      " \n",
      "And if you really care about accuracy, you should definitely try a bunch of different classifiers and select the best one by cross-validation. Or, to take a lesson from the Netflix Prize and Middle Earth, just use an ensemble method to choose them all!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1 Making tools easier to use.  Hadoop stack and NoSQLs really do require programming knowledge to unlock their power.\n",
      "Getting quicker answers across large data sets.  We can get them in \"acceptable\" amounts of time.  Its about getting that 3 hour query down to 5 minutes or less.  Cloudera Impala is a good example of work in this space.\n",
      "Integration with existing tools.  There's a few companies out there already working on this but I'm not seeing any real standards being developed for tight integration.  I think that eventually how you query data should be seamless to the person querying it, whether it is in a big data solution, RDBMS, JSON/XML, etc.\n",
      "Better security models.  There is almost no security in place for virtually all big data tools.  Once you get access, you get access to everything.  Improvements are being made, but it still isn't enterprise grade.\n",
      "Defining best practices for developing and using big data tools.\n",
      "Creating industry solutions that utilize big data.  We already see this happening in a couple of places, like utilities and healthcare, but it isn't yet wide spread.\n",
      "Defining that use case that isn't analyzing web based information that all enterprises can leverage.  Right now, most big data use cases are centered around solving problems with something involving the web.\n",
      "More mature software.  Right now, Hadoop logs are riddled with errors, warnings, and numerous other things that are almost impossible to decipher.  Yet, the damn thing still seems to magically work.  There needs to be improvements in better predicting and avoiding problems when running map-reduce jobs and giving human readable information to solve problems.\n",
      "Defining what big data actually is.\n",
      "Adoption.  I'm still not seeing truly widespread adoption of big data tools in the enterprise .  There's a lot of department solutions, technical solutions or proof of concepts.  But most haven't adopted it in the same way as companies like Facebook or Ebay.  And I don't see it happening anytime soon.  In the cases I've seen where big data solutions were considered alongside MPPs (Teradata, Exadata, etc), the MPPs win.  Once you get beyond the hype, there's a lot of cost to building out, maintaining, and developing a big data application.  And most companies just don't have teams of Java developers, data scientists, and distributed systems architects to really put those solutions to work."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Random forest"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}