{
 "metadata": {
  "name": "",
  "signature": "sha256:cd44a8f0c98d97858462fa25d995d25c743a8097c169d6f330df1c2e3d878daa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "R training tutorial\n",
      "===\n",
      "Chen Tong\n",
      "===\n",
      "\n",
      "Welcome to my tutorial, enjoy it. \n",
      "\n",
      "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/2.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 2.0 Generic License</a>.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name\n",
      "head\n",
      "table\n",
      "summary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "k-means\u8d8b\u52bf\u805a\u7c7b"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Take your original expression matrix [genes x conditions] and unitise the vectors, that is make the matrix magnitude invariant. The way to do this is to normalise the rows by the length of their vectors\n",
      "```R\n",
      "#for an expression matrix sim_class, get the sqrt(sum of squares) for each row\n",
      "norm_factors <- sqrt(apply(sim_class^2,1,sum));\n",
      "\n",
      "#divide the rows by the norm factor\n",
      "normalised_sim_class <- sim_class/norm_factors;\n",
      "```\n",
      "\n",
      "```R\n",
      "m2_scale <- t(apply(m2,1,scale))\n",
      "```\n",
      "Now clustering this expression matrix pulls out genes that have the same shape irrespective of magnitude. Biologically this means you are pulling together genes that might be direct targets of a particular set of transcription factors, with identical profiles but simply a different response scale. This is what you often find in reality, it is more often the case that co-regulated genes are responding in different scales, but in our experience with similar expression profile shapes. I hope that's of some use, even if only to run alongside your current analyses to see the differences this approach produces."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Co-expression netwrok"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "http://mpba.fbk.eu/en/home\n",
      "https://renette.fbk.eu/about/\n",
      "http://bioinformatics.lu/CoExpress/\n",
      "http://www.nature.com/ni/journal/v15/n2/box/ni.2787_BX2.html"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Spearman and pearson correlation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pearson's correlation is a measure of the linear relationship between two continuous random variables. It does not assume normality although it does assume finite variances and finite covariance. When the variables are bivariate normal, Pearson's correlation provides a complete description of the association.\n",
      "\n",
      "Spearman's correlation applies to ranks and so provides a measure of a monotonic relationship between two continuous random variables. It is also useful with ordinal data and is robust to outliers (unlike Pearson's correlation).\n",
      "\n",
      "If you want to explore your data it is best to compute both, since the relation between the Spearman (S) and Pearson (P) correlations will give some information. Briefly, `S` is computed on `ranks` and so depicts `monotonic` relationships while `P` is on `true values` and depicts `linear relationships`.\n",
      "\n",
      "As an example, if you set:\n",
      "```\n",
      "> x <- 1:100\n",
      "> y <- exp(x)\n",
      "> cor.test(x, y, method='pearson')  # coorelation 0.25\n",
      "\n",
      "\tPearson's product-moment correlation\n",
      "\n",
      "data:  x and y\n",
      "t = 2.5782, df = 98, p-value = 0.01142\n",
      "alternative hypothesis: true correlation is not equal to 0\n",
      "95 percent confidence interval:\n",
      " 0.05851039 0.42729737\n",
      "sample estimates:\n",
      "     cor \n",
      "0.252032 \n",
      "\n",
      "> ?cor.test\n",
      "> cor.test(x, y, method=\"spearman\")  # coorelation 1\n",
      "\n",
      "\tSpearman's rank correlation rho\n",
      "\n",
      "data:  x and y\n",
      "S = 0, p-value < 2.2e-16\n",
      "alternative hypothesis: true rho is not equal to 0\n",
      "sample estimates:\n",
      "rho \n",
      "  1 \n",
      "> cor.test(x,log(y), method='pearson')\n",
      "\n",
      "\tPearson's product-moment correlation\n",
      "\n",
      "data:  x and log(y)\n",
      "t = Inf, df = 98, p-value < 2.2e-16\n",
      "alternative hypothesis: true correlation is not equal to 0\n",
      "95 percent confidence interval:\n",
      " 1 1\n",
      "sample estimates:\n",
      "cor \n",
      "  1 \n",
      "```\n",
      "\n",
      "This is because y increases monotonically with x so the Spearman correlation is perfect, but not linearly, so the Pearson correlation is imperfect.\n",
      "\n",
      "Doing both is interesting because if you have `S > P`, that means that you have a correlation that is `monotonic` but not `linear`. Since it is good to have linearity in statistics (it is easier) you can try to apply a `transformation on y` (such a log). If you have `S < P`, this may indicate extreme outliers are affecting the correlation computation.\n",
      "\n",
      "The Spearman correlation is just the Pearson correlation using the ranks (order statistics) instead of the actual numeric values. The answer to your question is that they're not measuring the same thing. Pearson: linear trend, Spearman: monotonic trend. That the Pearson correlation is higher just means the linear correlation is larger than the rank correlation. This is probably due to `influential observations in the tails of the distribution that have large influence relative to their ranked values`. Tests of association using the Pearson correlation are of higher power when the linearity holds in the data."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}