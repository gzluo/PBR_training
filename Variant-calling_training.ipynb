{
 "metadata": {
  "name": "",
  "signature": "sha256:296571aefa1e5fbdaddcc0301c83e95803d695e43c5b71f1bcf86d2d6204f92f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Variant-calling training tutorial\n",
      "===\n",
      "Chen Tong\n",
      "===\n",
      "\n",
      "Welcome to my tutorial, enjoy it. \n",
      "\n",
      "This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/2.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 2.0 Generic License</a>.\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Next-Generation Sequencing basic knowledges"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Please refer to `NGS-basic_training.ipynb` for basic sequencing concepts and software installation and uasage."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exome-Seq"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Basic data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Please see http://gatkforums.broadinstitute.org/discussion/1204/what-input-files-does-the-gatk-accept-require.\n",
      "* Assume we have two compressed fastq file `NA19129.fq.gz` and `NA19240.fq.gz`.\n",
      "* Download genome data and SNP data from [GATK bundle](ftp://ftp.broadinstitute.org/bundle) \n",
      "    * Login to GATK bundle to check the latest version number by `ncftp -u gsapubftp-anonymous -p '<blank>' ftp.broadinstitute.org/bundle`.\n",
      "    * Download all data using `wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/`. Pay attention to version number `2.8` and genome assemble version `hg19`.\n",
      "    * Download only needed data as listed below:\n",
      "```\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/1000G_omni2.5.hg19.vcf.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/1000G_omni2.5.hg19.vcf.idx.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/1000G_phase1.indels.hg19.vcf.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/1000G_phase1.indels.hg19.vcf.idx.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/1000G_phase1.snps.high_confidence.hg19.vcf.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/1000G_phase1.snps.high_confidence.hg19.vcf.idx.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/CEUTrio.HiSeq.WGS.b37.bestPractices.hg19.vcf.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/dbsnp_138.hg19.vcf.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/dbsnp_138.hg19.vcf.idx.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/hapmap_3.3.hg19.vcf.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/hapmap_3.3.hg19.vcf.idx.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/Mills_and_1000G_gold_standard.indels.hg19.vcf.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/Mills_and_1000G_gold_standard.indels.hg19.vcf.idx.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/snp138CodingDbSnp.txt.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/ucsc.hg19.dict.gz\n",
      "wget -nd --user=gsapubftp-anonymous -r ftp://ftp.broadinstitute.org/bundle/2.8/hg19/ucsc.hg19.fasta.gz\n",
      "```\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Reads mapping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here using [BWA](http://bio-bwa.sourceforge.net/) to map reads to genome."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Constructing genome index"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Make sure the chromosoe order in `hg19.fasta` is the same as [GATK recommended](http://gatkforums.broadinstitute.org/discussion/1204/what-input-files-does-the-gatk-accept-require). Usually you can just download `hg19.fasta` from BATK bundle as mentioned before.\n",
      "* Two types of index are needed, one is `bwa` index, the other is `faidx`.\n",
      "* `-a` indicates the method used to construct BWA index. For large genome `bwtsw` should be given."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "bwa index -a bwtsw hg19.fasta -p hg19.fasta\n",
      "samtools faidx hg19.fasta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Single-END reads mapping"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The usage of `bwa` is like `bwa aln bwa_index fastq`.\n",
      "* `-t` tells the number of threads to use.\n",
      "* `-r` [RG-line] will be put in the generated SAM file specifying the ID, the library name and the sample name of the sequences and the sequencing platform. This is needed for GATK. Normally you only need to give the filename (without suffix) to `ID`, `LB` and `SM`. For me, `PL` is always `ILLUMINA`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "bwa aln -t 10 hg19.fasta NA19129.fq.gz >NA19129.sai 2>>NA19129.log\n",
      "bwa samse -r \"@RG\\tID:NA19129\\tLB:NA19129\\tSM:NA19129\\tPL:ILLUMINA\" \\\n",
      "    hg19.fasta NA19129.sai NA19129.fq.gz | gzip > NA19129/NA19129.sam.gz 2>>NA19129.log\n",
      "/bin/rm -f NA19129.sai\n",
      "bwa aln -t 10 hg19.fasta NA19240.fq.gz >NA19240.sai 2>>NA19240.log\n",
      "bwa samse -r \"@RG\\tID:NA19240\\tLB:NA19240\\tSM:NA19240\\tPL:ILLUMINA\" \\\n",
      "    hg19.fasta NA19240.sai NA19240.fq.gz | gzip > NA19240/NA19240.sam.gz 2>>NA19240.log\n",
      "/bin/rm -f NA19240.sai"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Paired-END sequencing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "bwa aln -t 10 hg19.fasta NA19129_1.fq.gz >NA19129_1.sai 2>>NA19129.log\n",
      "bwa aln -t 10 hg19.fasta NA19129_2.fq.gz >NA19129_2.sai 2>>NA19129.log\n",
      "bwa sampe -r \"@RG\\tID:NA19129\\tLB:NA19129\\tSM:NA19129\\tPL:ILLUMINA\" \\\n",
      "    hg19.fasta NA19129_1.sai NA19129_2.sai NA19129_1.fq.gz NA19129_2.fq.gz \\\n",
      "    | gzip > NA19129/NA19129.sam.gz 2>>NA19129.log\n",
      "/bin/rm -f NA19129_1.sai NA19129_2.sai &\n",
      "bwa aln -t 10 hg19.fasta NA19240_1.fq.gz >NA19240_1.sai 2>>NA19240.log\n",
      "bwa aln -t 10 hg19.fasta NA19240_2.fq.gz >NA19240_2.sai 2>>NA19240.log\n",
      "bwa sampe -r \"@RG\\tID:NA19240\\tLB:NA19240\\tSM:NA19240\\tPL:ILLUMINA\" \\\n",
      "    hg19.fasta NA19240_1.sai NA19240_2.sai NA19240_1.fq.gz NA19240_2.fq.gz \\\n",
      "    | gzip > NA19240/NA19240.sam.gz 2>>NA19240.log\n",
      "/bin/rm -f NA19240_1.sai NA19240_2.sai &"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Count the number of mapped reads"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "echo \"#NA19129 total reads: `samtools view -cS NA19129/NA19129.sam.gz`\"\n",
      "echo \"#NA19129 total mapped reads: `samtools view -cS -F4 NA19129/NA19129.sam.gz`\"\n",
      "echo \"#NA19240 total reads: `samtools view -cS NA19240/NA19240.sam.gz`\"\n",
      "echo \"#NA19240 total mapped reads: `samtools view -cS -F4 NA19240/NA19240.sam.gz`\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Mark duplicate reads"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Duplicates originate mostly from DNA prep methods and cause biases that skew variant calling results.\n",
      "* Here we use [`picard`](http://picard.sourceforge.net/index.shtml) to transfer mapping result , mark `duplicates` and output sorted `BAM` file with index created.\n",
      "* `-Xmx8g`: set the maximum allowed memory for this program.\n",
      "* `-jar`: list the commaned to use.\n",
      "* `SO=coordinate`: sort BAM file by reads coordinate.\n",
      "* `VALIDATION_STRINGENCY=LENIENT`: ignore some validation error in BAM files.\n",
      "* `CREATE_INDEX=true`: construct index for BAM files."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/picard/SortSam.jar SO=coordinate \\\n",
      "    INPUT=NA19129/NA19129.sam.gz OUTPUT=NA19129/NA19129.bam \\\n",
      "    VALIDATION_STRINGENCY=LENIENT >>NA19129.log 2>&1\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/picard/MarkDuplicates.jar \\\n",
      "    INPUT=NA19129/NA19129.bam OUTPUT=NA19129/NA19129.dedup.bam \\\n",
      "    METRICS_FILE=NA19129/NA19129.dedup.log CREATE_INDEX=true \\\n",
      "    VALIDATION_STRINGENCY=LENIENT >>NA19129.log 2>&1\n",
      "\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/picard/SortSam.jar SO=coordinate \\\n",
      "    INPUT=NA19240/NA19240.sam.gz OUTPUT=NA19240/NA19240.bam \\\n",
      "    VALIDATION_STRINGENCY=LENIENT >>NA19240.log 2>&1\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/picard/MarkDuplicates.jar \\\n",
      "    INPUT=NA19240/NA19240.bam OUTPUT=NA19240/NA19240.dedup.bam \\\n",
      "    METRICS_FILE=NA19240/NA19240.dedup.log CREATE_INDEX=true \\\n",
      "    VALIDATION_STRINGENCY=LENIENT >>NA19240.log 2>&1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Re-align indels and re-calibrate base quality score"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* InDels in reads (expecially near the ends) can trick the mappers to mis-aligning with mismatches.\n",
      "* These artifical mismatches can harm base quality recalibration and variant detection.These\n",
      "* Realignment around indels improve the accuracy of several of the downstream processing steps.\n",
      "* Three types of realignment targets:\n",
      "    * Known sites (e.g. dbSNP, 1000 Genomes)\n",
      "    * Indels seen in original alignments (in CIGARS)\n",
      "    * Sites where evidence suggests a hidden indel\n",
      "* How to generate known sites if not exist?\n",
      "    * \u9996\u5148\uff0c\u5148\u4f7f\u7528\u6ca1\u6709\u7ecf\u8fc7\u77eb\u6b63\u7684\u6570\u636e\u8fdb\u884c\u4e00\u8f6eSNP calling\uff1b\u7136\u540e\uff0c\u6311\u9009\u6700\u53ef\u4fe1\u7684SNP\u4f4d\u70b9\u8fdb\u884cBQSR\u5206\u6790\uff1b\u6700\u540e\uff0c\u5728\u4f7f\u7528\u8fd9\u4e9b\u7ecf\u8fc7BQSR\u7684\u6570\u636e\u8fdb\u884c\u4e00\u6b21\u771f\u6b63\u7684SNP calling\u3002\u8fd9\u51e0\u6b65\u53ef\u80fd\u8981\u91cd\u590d\u597d\u591a\u6b21\u624d\u80fd\u5f97\u5230\u53ef\u9760\u7684\u7ed3\u679c\u3002\n",
      "    * http://gatkforums.broadinstitute.org/discussion/1243/what-are-the-standard-resources-for-non-human-genomes\n",
      "    * http://gatkforums.broadinstitute.org/discussion/1243/what-are-the-standard-resources-for-non-human-genomes\n",
      "    http://gatkforums.broadinstitute.org/discussion/1783/variantrecalibrator-creating-a-truth-data-set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# Identify what regions to be aligned\n",
      "# Input BAM file not necassary if processing only at known indels.\n",
      "# Using a list of known indels will speed up processing and improve accuracy, \n",
      "# but is not necessary.\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T RealignerTargetCreator \\\n",
      "    -nt 10 -R hg19.fasta -known Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -known 1000G_phase1.indels.hg19.vcf -I NA19129/NA19129.dedup.bam \\\n",
      "    -o NA19129/NA19129.dedup.bam.interval.list >>NA19129.log 2>&1\n",
      "#Perform actual alignment\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T IndelRealigner \\\n",
      "    -R hg19.fasta -known Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -known 1000G_phase1.indels.hg19.vcf \\\n",
      "    -targetIntervals NA19129/NA19129.dedup.bam.interval.list \\\n",
      "    -I NA19129/NA19129.dedup.bam -o NA19129/NA19129.dedup.realigned.bam >>NA19129.log 2>&1\n",
      "    \n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T RealignerTargetCreator \\\n",
      "    -nt 10 -R hg19.fasta -known Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -known 1000G_phase1.indels.hg19.vcf -I NA19240/NA19240.dedup.bam \\\n",
      "    -o NA19240/NA19240.dedup.bam.interval.list >>NA19240.log 2>&1\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T IndelRealigner \\\n",
      "    -R hg19.fasta -known Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -known 1000G_phase1.indels.hg19.vcf \\\n",
      "    -targetIntervals NA19240/NA19240.dedup.bam.interval.list \\\n",
      "    -I NA19240/NA19240.dedup.bam -o NA19240/NA19240.dedup.realigned.bam >>NA19240.log 2>&1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Quality scores are critical for all downstream analysis.\n",
      "* Systematic biases are a major contributor to bad calls.\n",
      "* Unfortunatelly, the quality scores issued by sequencers are inaccurate and biased.\n",
      "* Base Quality Score Recalibration provides a calibrated error model from which to make mutation calls.\n",
      "    * Per base indel error rate also varies by lane, sequence context and sequencing technology.\n",
      "    * Empirical estimates of base insertion and base deletion error rates unify SNP and indel error models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "#Model the error modes and recalibrate qualities\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T BaseRecalibrator \\\n",
      "    -R hg19.fasta -knownSites dbsnp_138.hg19.vcf \\\n",
      "    -knownSites Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -knownSites 1000G_phase1.indels.hg19.vcf  \\\n",
      "    -I NA19129/NA19129.dedup.realigned.bam \\\n",
      "    -o NA19129/NA19129.dedup.realigned.before_recal.table >>NA19129.log 2>&1\n",
      "#Write recalibrated data to file\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T PrintReads \\\n",
      "    -R hg19.fasta -I NA19129/NA19129.dedup.realigned.bam \\\n",
      "    -BQSR NA19129/NA19129.dedup.realigned.before_recal.table \\\n",
      "    -o NA19129/NA19129.dedup.realigned.recal.bam >>NA19129.log 2>&1\n",
      "\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T BaseRecalibrator \\\n",
      "    -R hg19.fasta -knownSites dbsnp_138.hg19.vcf \\\n",
      "    -knownSites Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -knownSites 1000G_phase1.indels.hg19.vcf  \\\n",
      "    -I NA19240/NA19240.dedup.realigned.bam \\\n",
      "    -o NA19240/NA19240.dedup.realigned.before_recal.table >>NA19240.log 2>&1\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T PrintReads \\\n",
      "    -R hg19.fasta -I NA19240/NA19240.dedup.realigned.bam \\\n",
      "    -BQSR NA19240/NA19240.dedup.realigned.before_recal.table \\\n",
      "    -o NA19240/NA19240.dedup.realigned.recal.bam >>NA19240.log 2>&1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "#Generate quality score for re-calibrated BAM\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T BaseRecalibrator \\\n",
      "    -R hg19.fasta -knownSites dbsnp_138.hg19.vcf \\\n",
      "    -knownSites Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -knownSites 1000G_phase1.indels.hg19.vcf -I NA19129/NA19129.dedup.realigned.bam \\\n",
      "    -BQSR NA19129/NA19129.dedup.realigned.before_recal.table \\\n",
      "    -o NA19129/NA19129.dedup.realigned.after_recal.table >>NA19129.log 2>&1\n",
      "#Quality score plot before and after recalibration\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T AnalyzeCovariates \\\n",
      "    -R hg19.fasta -before NA19129/NA19129.dedup.realigned.before_recal.table \\\n",
      "    -after NA19129/NA19129.dedup.realigned.after_recal.table \\\n",
      "    -plots NA19129/NA19129.dedup.realigned.recal_plot.pdf \\\n",
      "    -csv NA19129/NA19129.dedup.realigned.recal_plot.csv >>NA19129.log 2>&1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data compression with reduced reads"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Reducing the size of BAM files allowing greater running performence.\n",
      "* Read\u2010based compression keeps only essential information for variant calling.\n",
      "* How to judge if compression work properly\n",
      "    * Reads should be stripped out of all extra tags in the BAM file.\n",
      "    * A quick variant calling run on a small region of the genome on both full and reduced BAM and look for highly similar variant calls.\n",
      "       * If number s are too disparate (either compressed BAM is missing variants or is carrying many new variants), a more in-depth look at the file is advised.\n",
      "       * Coverage test with `DiagnoseTargets` should yield similar results for variant regions and capped results for concensus regions.\n",
      "* It is not necessary to do this step when calling variants using `HaplotypeCaller` or there is very few samples. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T ReduceReads \\\n",
      "    -R hg19.fasta -I NA19129/NA19129.dedup.realigned.recal.bam \\\n",
      "    -O NA19129/NA19129.dedup.realigned.recal.reduced.bam "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SNP calling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Genetic variant or random machine noise = large scale Bayesian inference problem\n",
      "* `UnifiedGenotyper`(shorted as `UG`)\n",
      "    * Call SNPs and indels separately by considering each variant locus independently\n",
      "    * Accepts any ploidy\n",
      "    * Polled calling\n",
      "    * High sample numbers\n",
      "* `HaplotypeCaller`(shorted as `HC`)\n",
      "    * Call SNPs, indels, and some SVs simultaneously by performing a local de-novo assembly\n",
      "    * More accurate, especially for indels\n",
      "    * Will eventually replace `UG`\n",
      "* Tips for accelrating the running process\n",
      "    * Reduce BAM\n",
      "    * The command line argument which most determines the runtime of the `HaplotypeCaller` is `-minPruning`. This argument controls the amount of pruning that is performed on the local de-novo assembly graph which is generated for every variant region. The default value is `-minPruning 1` which means events have to been seen by more than 1 read in order to remain in the graph. This is a very conservative default designed not to miss anything. By raising this threshold fewer haplotypes will need to be evaluated and this reduces the runtime. The optimal value for this parameter depends on your project design and so experimentation is required."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# For Exome-Seq\n",
      "# -L: contains a list of the targeted intervals we wanted to sequence. \n",
      "# Normally we can get this interval file from the Producter of the capture-chip \n",
      "# we used. \n",
      "# Otherwise we can get this from UCSC--Table brower by adding 10 or 20 bp to annotated\n",
      "# exon regions.\n",
      "# Usually I will supply a BED format file to HC. It also accepts files in other format.\n",
      "# Plase check the link in http://gatkforums.broadinstitute.org/discussion/4133/when-should-i-use-l-to-pass-in-a-list-of-intervals#latest to see in detail.\n",
      "\n",
      "# variant calling for each sample individually\n",
      "\n",
      "# \u2013genotyping_mode: This specifies how we want the program to determine the alternate \n",
      "#    alleles to use for genotyping. \n",
      "#    In the default DISCOVERY mode, the program will choose the most likely alleles \n",
      "#    out of those it sees in the data. \n",
      "#    In GENOTYPE_GIVEN_ALLELES mode, the program will only use the alleles passed in \n",
      "#    from a VCF file (using the -alleles argument). \n",
      "#    This is useful if you just want to determine if a sample has a specific genotype of \n",
      "#    interest and you are not interested in other alleles.\n",
      "\n",
      "# \u2013stand_emit_conf: Emission confidence threshold. \n",
      "#    This is the minimum confidence threshold (phred-scaled) at which the program should \n",
      "#    emit sites that appear to be possibly variant.\n",
      "\n",
      "# \u2013stand_call_conf: Calling confidence threshold\n",
      "#    This is the minimum confidence threshold (phred-scaled) at which the program should \n",
      "#    emit variant sites as called. If a site's associated genotype has a confidence \n",
      "#    score lower than the calling threshold, the program will emit the site as filtered \n",
      "#    and will annotate it as LowQual. This threshold separates high confidence calls \n",
      "#    from low confidence calls.\n",
      "\n",
      "# The terms called and filtered are tricky because they can mean different things \n",
      "# depending on context. In ordinary language, people often say a site was called \n",
      "# if it was emitted as variant. But in the GATK's technical language, \n",
      "# saying a site was called means that that site passed the confidence threshold test. \n",
      "# For filtered, it's even more confusing, because in ordinary language, \n",
      "# when people say that sites were filtered, they usually mean that \n",
      "# those sites successfully passed a filtering test. \n",
      "# However, in the GATK's technical language, the same phrase (saying that sites were filtered)\n",
      "# means that those sites failed the filtering test. \n",
      "# In effect, it means that those would be filtered out if the filter was used to \n",
      "# actually remove low-confidence calls from the callset, instead of just tagging them. \n",
      "# In both cases, both usages are valid depending on the point of view of the person \n",
      "# who is reporting the results. So it's always important to check what is the context \n",
      "# when interpreting results that include these terms.\n",
      "\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T HaplotypeCaller \n",
      "    -R hg19.fasta -L hg19.refseqGene.exonPlus20.bed \\\n",
      "    -I NA19129/NA19129.dedup.realigned.recal.bam \\\n",
      "    -o NA19129/NA19129.dedup.realigned.recal.vcf \\\n",
      "    -stand_call_conf 30.0 -stand_emit_conf 10.0 -minPruning 3 --emitRefConfidence GVCF \\\n",
      "    --variant_index_type LINEAR --variant_index_parameter 128000 >>NA19129.log 2>&1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "# For Whole-Genome-Seq\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T HaplotypeCaller \n",
      "    -R hg19.fasta \\\n",
      "    -I NA19129/NA19129.dedup.realigned.recal.bam \\\n",
      "    -o NA19129/NA19129.dedup.realigned.recal.vcf \\\n",
      "    -stand_call_conf 30.0 -stand_emit_conf 10.0 -minPruning 3 --emitRefConfidence GVCF \\\n",
      "    --variant_index_type LINEAR --variant_index_parameter 128000 >>NA19129.log 2>&1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "#Merge all gvcfs generated for each sample into one gvcf\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T CombineGVCFs \\\n",
      "    -R hg19.fasta -o Exosome.mergeGVCF.vcf \\\n",
      "    --variant NA19129/NA19129.dedup.realigned.recal.vcf  \\\n",
      "    --variant NA19240/NA19240.dedup.realigned.recal.vcf >>merge_snp_call.log 2>&1\n",
      "\n",
      "#Joint genotyping\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK3/GenomeAnalysisTK.jar -T GenotypeGVCFs \\\n",
      "    -R hg19.fasta -o Exosome.genotypeGVCF.vcf \\\n",
      "    --variant Exosome.mergeGVCF.vcf >>merge_snp_call.log 2>&1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Raw callsets are often very large and full of false positive mutation calls. So further work is needed before this callset can be used for any meaningful analysis."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SNP filtering (hard filter)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Apply hard filters to a variant callset that is too small for VQSR or for which truth/training sets are not available.\n",
      "* http://gatkforums.broadinstitute.org/discussion/2806/howto-apply-hard-filters-to-a-call-set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Filtering recommendations for SNPs:\n",
      "    * QD < 2.0\n",
      "    * MQ < 40.0\n",
      "    * FS > 60.0\n",
      "    * HaplotypeScore > 13.0\n",
      "    * MQRankSum < -12.5\n",
      "    * ReadPosRankSum < -8.0\n",
      "* Filtering recommendations for indels:\n",
      "    * QD < 2.0\n",
      "    * ReadPosRankSum < -20.0\n",
      "    * InbreedingCoeff < -0.8\n",
      "    * FS > 200.0\n",
      "* And now some more IMPORTANT caveats (don't skip this!)\n",
      "\n",
      "    * The `InbreedingCoeff` statistic is a population-level calculation that is only available with `10 or more samples`. If you have fewer samples you will need to omit that particular filter statement.\n",
      "\n",
      "    * For shallow-coverage (<10x), it is virtually impossible to use manual filtering to reliably separate true positives from false positives. You really, really, really should use the protocol involving variant quality score recalibration. If you can't do that, maybe you need to take a long hard look at your experimental design. In any case you're probably in for a world of pain.\n",
      "\n",
      "    * The maximum `DP` (depth) filter only applies to whole genome data, where the probability of a site having exactly N reads given an average coverage of M is a well-behaved function. First principles suggest this should be a binomial sampling but in practice it is more a Gaussian distribution. Regardless, the DP threshold should be set a 5 or 6 sigma from the mean coverage across all samples, so that the DP > X threshold eliminates sites with excessive coverage caused by alignment artifacts. Note that for exomes, a straight DP filter shouldn't be used because the relationship between misalignments and depth isn't clear for capture data.\n",
      "\n",
      "* Ref http://www.broadinstitute.org/gatk/guide/article?id=3225"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "\n",
      "#For Exomse-Seq\n",
      "\n",
      "#Extract the SNPs from the call set\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T SelectVariants \\\n",
      "    -R hg19.fasta -selectType SNP -V Exosome.genotypeGVCF.vcf -o Exosome.genotypeGVCF.SNPs.vcf \\\n",
      "    >>hard_filter.log 2>&1\n",
      "#Determine and apply parameter for filtering SNPs\n",
      "\n",
      "# SNPs matching any of these conditions will be considered bad and filtered out, \n",
      "# i.e. marked FILTER in the output VCF file. \n",
      "# The program will specify which parameter was chiefly responsible for the exclusion \n",
      "# of the SNP using the culprit annotation. \n",
      "# SNPs that do not match any of these conditions will be considered good and marked PASS \n",
      "# in the output VCF file.\n",
      "\n",
      "# QualByDepth (QD) 2.0\n",
      "#    This is the variant confidence (from the QUAL field) divided by the unfiltered depth \n",
      "#    of non-reference samples.\n",
      "\n",
      "# FisherStrand (FS) 60.0\n",
      "#    Phred-scaled p-value using Fisher\u2019s Exact Test to detect strand bias \n",
      "#    (the variation being seen on only the forward or only the reverse strand) in the reads. \n",
      "#    More bias is indicative of false positive calls.\n",
      "\n",
      "# RMSMappingQuality (MQ) 40.0\n",
      "#    This is the Root Mean Square of the mapping quality of the reads across all samples.\n",
      "\n",
      "# HaplotypeScore 13.0\n",
      "#    This is the consistency of the site with two (and only two) segregating haplotypes. \n",
      "#    Note that this is not applicable for calls made using the \n",
      "#    UnifiedGenotyper on non-diploid organisms.\n",
      "\n",
      "# MappingQualityRankSumTest (MQRankSum) 12.5\n",
      "#    This is the u-based z-approximation from the Mann-Whitney Rank Sum Test \n",
      "#    for mapping qualities (reads with ref bases vs. those with the alternate allele). \n",
      "#    Note that the mapping quality rank sum test can not be calculated for \n",
      "#    sites without a mixture of reads showing both the reference and alternate alleles, \n",
      "#    i.e. this will only be applied to heterozygous calls.\n",
      "\n",
      "# ReadPosRankSumTest (ReadPosRankSum) 8.0\n",
      "#    This is the u-based z-approximation from the Mann-Whitney Rank Sum Test for \n",
      "#    the distance from the end of the read for reads with the alternate allele. \n",
      "#    If the alternate allele is only seen near the ends of reads, this is indicative of error. \n",
      "#    Note that the read position rank sum test can not be calculated for \n",
      "#    sites without a mixture of reads showing both the reference and alternate alleles, \n",
      "#    i.e. this will only be applied to heterozygous calls.\n",
      "\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T VariantFiltration \\\n",
      "    -R hg19.fasta \\\n",
      "    --filterExpression \"QD < 2.0\" --filterName \"LowQualByDepth\" \\\n",
      "    --filterExpression \"FS > 60.0\" --filterName \"HighFisherStrand\" \\\n",
      "    --filterExpression \"MQ < 40.0\" --filterName \"LowRMSMappingQuality\" \\\n",
      "    --filterExpression \"HaplotypeScore > 13.0\" --filterName \"HighHaplotypeScore\" \\\n",
      "    --filterExpression \"MappingQualityRankSum < -12.5\" --filterName \"LowMappingQualityRankSum\" \\\n",
      "    --filterExpression \"ReadPosRankSum < -8.0\" --filterName \"HighReadPosRankSum\" \\\n",
      "    -V Exosome.genotypeGVCF.SNPs.vcf -o Exosome.genotypeGVCF.SNPs.hardfilter.vcf \\\n",
      "    >>hard_filter.log 2>&1\n",
      "\n",
      "# Extract the indels from the call set\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T SelectVariants \\\n",
      "    -R hg19.fasta -selectType INDEL -V Exosome.genotypeGVCF.vcf \\\n",
      "    -o Exosome.genotypeGVCF.INDEL.vcf >>hard_filter.log 2>&1\n",
      "\n",
      "# Determine and apply parameter for filtering indels\n",
      "\n",
      "# Indels matching any of these conditions will be considered bad and filtered out, \n",
      "# i.e. marked FILTER in the output VCF file. The program will specify which parameter was \n",
      "# chiefly responsible for the exclusion of the indel using the culprit annotation. \n",
      "# Indels that do not match any of these conditions will be considered good and marked PASS \n",
      "# in the output VCF file.\n",
      "\n",
      "# QualByDepth (QD) 2.0\n",
      "#    This is the variant confidence (from the QUAL field) divided by the unfiltered depth \n",
      "#    of non-reference samples.\n",
      "\n",
      "# FisherStrand (FS) 200.0\n",
      "#    Phred-scaled p-value using Fisher\u2019s Exact Test to detect strand bias \n",
      "#    (the variation being seen on only the forward or only the reverse strand) in the reads. \n",
      "#    More bias is indicative of false positive calls.\n",
      "\n",
      "# ReadPosRankSumTest (ReadPosRankSum) 20.0\n",
      "#    This is the u-based z-approximation from the Mann-Whitney Rank Sum Test for the distance \n",
      "#    from the end of the read for reads with the alternate allele. \n",
      "#    If the alternate allele is only seen near the ends of reads, this is indicative of error. \n",
      "#    Note that the read position rank sum test can not be calculated for \n",
      "#    sites without a mixture of reads showing both the reference and alternate alleles, \n",
      "#    i.e. this will only be applied to heterozygous calls.\n",
      "\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T VariantFiltration \\\n",
      "    -R hg19.fasta \\\n",
      "    --filterExpression \"QD < 2.0\" --filterName \"LowQualByDepth\" \\\n",
      "    --filterExpression \"FS > 200.0\" --filterName \"HighFisherStrand\" \\\n",
      "    --filterExpression \"ReadPosRankSum < -20.0\" --filterName \"HighReadPosRankSum\" \\\n",
      "    -V Exosome.genotypeGVCF.INDEL.vcf -o Exosome.genotypeGVCF.INDEL.hardfilter.vcf \\\n",
      "    >>hard_filter.log 2>&1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SNP filtering (VQSR)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Recalibrate variant quality scores and produce a callset filtered for the desired levels of sensitivity and specificity.\n",
      "\n",
      "**For SNPs**\n",
      "\n",
      "* a. Specify which call sets the program should use as resources to build the recalibration model\n",
      "\n",
      "    * For each training set, we use key-value tags to qualify whether the set contains known sites, training sites, and/or truth sites. We also use a tag to specify the prior likelihood that those sites are true (using the Phred scale).\n",
      "\n",
      "    * True sites training resource: `HapMap`\n",
      "        * This resource is a SNP call set that has been validated to a very high degree of confidence. The program will consider that the variants in this resource are representative of true sites (`truth=true`), and will use them to train the recalibration model (`training=true`). We will also use these sites later on to choose a threshold for filtering variants based on sensitivity to truth sites. The prior likelihood we assign to these variants is Q15 (96.84%).\n",
      "\n",
      "    * True sites training resource: `Omni`\n",
      "        * This resource is a set of polymorphic SNP sites produced by the Omni genotyping array. The program will consider that the variants in this resource are representative of true sites (`truth=true`), and will use them to train the recalibration model (`training=true`). The prior likelihood we assign to these variants is Q12 (93.69%).\n",
      "\n",
      "    * Non-true sites training resource: `1000G`\n",
      "        * This resource is a set of high-confidence SNP sites produced by the 1000 Genomes Project. The program will consider that the variants in this resource may contain true variants as well as false positives (`truth=false`), and will use them to train the recalibration model (`training=true`). The prior likelihood we assign to these variants is Q10 (%).\n",
      "\n",
      "    * Known sites resource, not used in training: `dbSNP`\n",
      "        * This resource is a SNP call set that has not been validated to a high degree of confidence (`truth=false`). The program will not use the variants in this resource to train the recalibration model (`training=false`). However, the program will use these to stratify output metrics such as `Ti/Tv` ratio by whether variants are present in dbsnp or not (`known=true`). The prior likelihood we assign to these variants is Q2 (36.90%).\n",
      "\n",
      "    * \tThe default prior likelihood assigned to all other variants is Q2 (36.90%). This low value reflects the fact that the philosophy of the GATK callers is to produce a large, highly sensitive callset that needs to be heavily refined through additional filtering.\n",
      "\n",
      "* b. Specify which annotations the program should use to evaluate the likelihood of Indels being real\n",
      "    * These annotations are included in the information generated for each variant call by the caller. If an annotation is missing (typically because it was omitted from the calling command) it can be added using the VariantAnnotator tool.\n",
      "\n",
      "    * Coverage (DP)\n",
      "        * Total (unfiltered) depth of coverage.\n",
      "\n",
      "    * QualByDepth (QD)\n",
      "        * Variant confidence (from the QUAL field) / unfiltered depth of non-reference samples.\n",
      "\n",
      "    * FisherStrand (FS)\n",
      "        * Phred-scaled p-value using Fisher's Exact Test to detect strand bias (the variation being seen on only the forward or only the reverse strand) in the reads. More bias is indicative of false positive calls.\n",
      "\n",
      "    * MappingQualityRankSumTest (MQRankSum)\n",
      "        * The u-based z-approximation from the Mann-Whitney Rank Sum Test for mapping qualities (reads with ref bases vs. those with the alternate allele). Note that the mapping quality rank sum test can not be calculated for sites without a mixture of reads showing both the reference and alternate alleles.\n",
      "\n",
      "    * ReadPosRankSumTest (ReadPosRankSum)\n",
      "        * The u-based z-approximation from the Mann-Whitney Rank Sum Test for the distance from the end of the read for reads with the alternate allele. If the alternate allele is only seen near the ends of reads, this is indicative of error. Note that the read position rank sum test can not be calculated for sites without a mixture of reads showing both the reference and alternate alleles.\n",
      "\n",
      "* c. Specify the desired truth sensitivity threshold values that the program should use to generate tranches\n",
      "    * First tranche threshold 100.0\n",
      "\n",
      "    * Second tranche threshold 99.9\n",
      "\n",
      "    * Third tranche threshold 99.0\n",
      "\n",
      "    * Fourth tranche threshold 90.0\n",
      "\n",
      "    * Tranches are essentially slices of variants, ranked by VQSLOD, bounded by the threshold values specified in this step. The threshold values themselves refer to the sensitivity we can obtain when we apply them to the call sets that the program uses to train the model. The idea is that the lowest tranche is highly specific but less sensitive (there are very few false positives but potentially many false negatives, i.e. missing calls), and each subsequent tranche in turn introduces additional true positive calls along with a growing number of false positive calls. This allows us to filter variants based on how sensitive we want the call set to be, rather than applying hard filters and then only evaluating how sensitive the call set is using post hoc methods.\n",
      "\n",
      "**For INDELs**\n",
      "\n",
      "* a. Specify which call sets the program should use as resources to build the recalibration model\n",
      "    * For each training set, we use key-value tags to qualify whether the set contains known sites, training sites, and/or truth sites. We also use a tag to specify the prior likelihood that those sites are true (using the Phred scale).\n",
      "\n",
      "    * Known and true sites training resource: `Mills`\n",
      "        * This resource is an Indel call set that has been validated to a high degree of confidence. The program will consider that the variants in this resource are representative of true sites (`truth=true`), and will use them to train the recalibration model (`training=true`). The prior likelihood we assign to these variants is Q12 (93.69%).\n",
      "\n",
      "    * The default prior likelihood assigned to all other variants is Q2 (36.90%). This low value reflects the fact that the philosophy of the GATK callers is to produce a large, highly sensitive callset that needs to be heavily refined through additional filtering.\n",
      "\n",
      "* b. Specify which annotations the program should use to evaluate the likelihood of Indels being real\n",
      "    * These annotations are included in the information generated for each variant call by the caller. If an annotation is missing (typically because it was omitted from the calling command) it can be added using the VariantAnnotator tool.\n",
      "\n",
      "    * Coverage (DP)\n",
      "        * Total (unfiltered) depth of coverage.\n",
      "\n",
      "    * FisherStrand (FS)\n",
      "        * Phred-scaled p-value using Fisher's Exact Test to detect strand bias (the variation being seen on only the forward or only the reverse strand) in the reads. More bias is indicative of false positive calls.\n",
      "\n",
      "    * MappingQualityRankSumTest (MQRankSum)\n",
      "        * The u-based z-approximation from the Mann-Whitney Rank Sum Test for mapping qualities (reads with ref bases vs. those with the alternate allele). Note that the mapping quality rank sum test can not be calculated for sites without a mixture of reads showing both the reference and alternate alleles.\n",
      "\n",
      "    * ReadPosRankSumTest (ReadPosRankSum)\n",
      "        * The u-based z-approximation from the Mann-Whitney Rank Sum Test for the distance from the end of the read for reads with the alternate allele. If the alternate allele is only seen near the ends of reads, this is indicative of error. Note that the read position rank sum test can not be calculated for sites without a mixture of reads showing both the reference and alternate alleles.\n",
      "\n",
      "* c. Specify the desired truth sensitivity threshold values that the program should use to generate tranches\n",
      "    * First tranche threshold 100.0\n",
      "\n",
      "    * Second tranche threshold 99.9\n",
      "\n",
      "    * Third tranche threshold 99.0\n",
      "\n",
      "    * Fourth tranche threshold 90.0\n",
      "\n",
      "    * Tranches are essentially slices of variants, ranked by VQSLOD, bounded by the threshold values specified in this step. The threshold values themselves refer to the sensitivity we can obtain when we apply them to the call sets that the program uses to train the model. The idea is that the lowest tranche is highly specific but less sensitive (there are very few false positives but potentially many false negatives, i.e. missing calls), and each subsequent tranche in turn introduces additional true positive calls along with a growing number of false positive calls. This allows us to filter variants based on how sensitive we want the call set to be, rather than applying hard filters and then only evaluating how sensitive the call set is using post hoc methods.\n",
      "\n",
      "* d. Determine additional model parameters\n",
      "    * Maximum number of Gaussians (-maxGaussians) 4\n",
      "        * This is the maximum number of Gaussians (i.e. clusters of variants that have similar properties) that the program should try to identify when it runs the variational Bayes algorithm that underlies the machine learning method. In essence, this limits the number of different \u201dprofiles\u201d of variants that the program will try to identify. This number should only be increased for datasets that include very many variants."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Whole-Genome-Seq\u4f7f\u7528\u7684\u53c2\u6570\u548c\u9700\u8981\u6ce8\u610f\u7684\u4e8b\u9879"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "#Whole-Genome-Seq for more than 9 samples (-an InbreedingCoeff)\n",
      "#VariantRecalibrator for SNP\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T VariantRecalibrator \\\n",
      "    -R hg19.fasta -input Exosome.genotypeGVCF.vcf -mode SNP \\\n",
      "    -resource:hapmap,known=false,training=true,truth=true,prior=15.0 hapmap_3.3.hg19.vcf \\\n",
      "    -resource:omni,known=false,training=true,truth=true,prior=12.0 1000G_omni2.5.hg19.vcf \\\n",
      "    -resource:1000G,known=false,training=true,truth=false,prior=10.0 1000G_phase1.snps.high_confidence.hg19.vcf \\\n",
      "    -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 dbsnp_138.hg19.vcf \\\n",
      "    -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an DP -an InbreedingCoeff \\\n",
      "    -recalFile Exosome.genotypeGVCF.SNPs.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs.vqsr_tranches \\\n",
      "    -rscriptFile Exosome.genotypeGVCF.SNPs.plots.r \\\n",
      "    -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 -nt 10 >>vqsr_filter.log 2>&1\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T ApplyRecalibration \\\n",
      "    -R hg19.fasta -input Exosome.genotypeGVCF.vcf -mode SNP \\\n",
      "    -ts_filter_level 99.5 -recalFile Exosome.genotypeGVCF.SNPs.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs.vqsr_tranches \\\n",
      "    -o Exosome.genotypeGVCF.SNPs.vqsr.vcf >>vqsr_filter.log 2>&1\n",
      "\n",
      "#VariantRecalibrator for INDELS\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T VariantRecalibrator \\\n",
      "    -R hg19.fasta -mode INDEL -input Exosome.genotypeGVCF.SNPs.vqsr.vcf --maxGaussians 4 \\\n",
      "    -resource:mills,known=false,training=true,truth=true,prior=12.0 Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 dbsnp_138.hg19.vcf \\\n",
      "    -an QD -an FS -an ReadPosRankSum -an MQRankSum -an DP -an InbreedingCoeff \\\n",
      "    -recalFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_tranches \\\n",
      "    -rscriptFile Exosome.genotypeGVCF.SNPs_INDEL.plots.r -nt 10  >>vqsr_filter.log 2>&1\n",
      "    \n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T ApplyRecalibration \\\n",
      "    -R hg19.fasta -input Exosome.genotypeGVCF.SNPs.vqsr.vcf -mode INDEL \\\n",
      "    -ts_filter_level 99.0 -recalFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_tranches \\\n",
      "    -o Exosome.genotypeGVCF.SNPs_INDEL.vqsr.vcf >>vqsr_filter.log 2>&1; \\\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Exome-Seq\u4f7f\u7528\u7684\u53c2\u6570\u548c\u9700\u8981\u6ce8\u610f\u7684\u4e8b\u9879"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Depth of coverage (the DP annotation invoked by Coverage) should **not** be used when working with **exome** datasets since there is extreme variation in the depth to which targets are captured! In whole genome experiments this variation is indicative of error but that is not the case in capture experiments.\n",
      "* The *InbreedingCoeff* is a population level statistic that requires at least **10** samples in order to be computed. For projects with fewer samples please omit this annotation from the command line.\n",
      "* In our testing we've found that in order to achieve the best exome results one needs to use an exome SNP and/or indel callset with at least **30** samples. For users with experiments containing fewer exome samples there are several options to explore:\n",
      "\n",
      "    * Add additional samples for variant calling, either by sequencing additional samples or using publicly available exome bams from the 1000 Genomes Project (this option is used by the Broad exome production pipeline)\n",
      "    * Use the VQSR with the smaller variant callset but experiment with the precise argument settings (try adding --maxGaussians 4 to your command line, for example)\n",
      "\n",
      "\n",
      "* \u53c2\u8003\u8d44\u6599\n",
      "    * http://gatkforums.broadinstitute.org/discussion/1259/what-vqsr-training-sets-arguments-should-i-use-for-my-specific-project \n",
      "    * http://gatkforums.broadinstitute.org/discussion/1783/variantrecalibrator-creating-a-truth-data-set\n",
      "    * http://www.broadinstitute.org/gatk/guide/article?id=39\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "#Exomse-Seq for more than 9 samples (-an InbreedingCoeff)\n",
      "#VariantRecalibrator for SNP\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T VariantRecalibrator \\\n",
      "    -R hg19.fasta -input Exosome.genotypeGVCF.vcf -mode SNP \\\n",
      "    -resource:hapmap,known=false,training=true,truth=true,prior=15.0 hapmap_3.3.hg19.vcf \\\n",
      "    -resource:omni,known=false,training=true,truth=true,prior=12.0 1000G_omni2.5.hg19.vcf \\\n",
      "    -resource:1000G,known=false,training=true,truth=false,prior=10.0 1000G_phase1.snps.high_confidence.hg19.vcf \\\n",
      "    -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 dbsnp_138.hg19.vcf \\\n",
      "    -an QD -an MQ -an MQRankSum -an ReadPosRankSum -an FS -an InbreedingCoeff \\\n",
      "    -recalFile Exosome.genotypeGVCF.SNPs.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs.vqsr_tranches \\\n",
      "    -rscriptFile Exosome.genotypeGVCF.SNPs.plots.r \\\n",
      "    -tranche 100.0 -tranche 99.9 -tranche 99.0 -tranche 90.0 -nt 10 >>vqsr_filter.log 2>&1\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T ApplyRecalibration \\\n",
      "    -R hg19.fasta -input Exosome.genotypeGVCF.vcf -mode SNP \\\n",
      "    -ts_filter_level 99.5 -recalFile Exosome.genotypeGVCF.SNPs.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs.vqsr_tranches \\\n",
      "    -o Exosome.genotypeGVCF.SNPs.vqsr.vcf >>vqsr_filter.log 2>&1\n",
      "\n",
      "#VariantRecalibrator for INDELS\n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T VariantRecalibrator \\\n",
      "    -R hg19.fasta -mode INDEL -input Exosome.genotypeGVCF.SNPs.vqsr.vcf --maxGaussians 4 \\\n",
      "    -resource:mills,known=false,training=true,truth=true,prior=12.0 Mills_and_1000G_gold_standard.indels.hg19.vcf \\\n",
      "    -resource:dbsnp,known=true,training=false,truth=false,prior=2.0 dbsnp_138.hg19.vcf \\\n",
      "    -an QD -an FS -an ReadPosRankSum -an MQRankSum -an InbreedingCoeff \\\n",
      "    -recalFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_tranches \\\n",
      "    -rscriptFile Exosome.genotypeGVCF.SNPs_INDEL.plots.r -nt 10  >>vqsr_filter.log 2>&1\n",
      "    \n",
      "java -Xmx8g -Djava.io.tmpdir=/tmp -jar ~/GATK/GenomeAnalysisTK.jar -T ApplyRecalibration \\\n",
      "    -R hg19.fasta -input Exosome.genotypeGVCF.SNPs.vqsr.vcf -mode INDEL \\\n",
      "    -ts_filter_level 99.0 -recalFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_recal \\\n",
      "    -tranchesFile Exosome.genotypeGVCF.SNPs_INDEL.vqsr_tranches \\\n",
      "    -o Exosome.genotypeGVCF.SNPs_INDEL.vqsr.vcf >>vqsr_filter.log 2>&1; \\\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "\u53c2\u6570\u89e3\u91ca"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* `-ts_filter_level`: We typically seek to achieve 99.5% sensitivity to the accessible truth sites for SNPs and 99.0% for INDELs.\n",
      "* `-tranche`: The default values are 100.0, 99.9, 99.0, and 90.0"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "\u9519\u8bef\u7c7b\u578b\u548c\u89e3\u51b3\u65b9\u5f0f"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* ERROR MESSAGE: Timeout of 30000 milliseconds was reached while trying to acquire a lock on file 1000G_phase1.snps.high_confidence.hg19.vcf.idx. Since the GATK uses non-blocking lock acquisition calls that are not supposed to wait, this implies a problem with the file locking support in your operating system.\n",
      "    * \u6dfb\u52a0\u53c2\u6570 --disable_auto_index_creation_and_locking_when_reading_rods \n",
      "        * [This parameter disables index auto-creation and related file locking when reading vcfs. If all index files are pre-existing, and no concurrent processes will ever update any of the indices, it should be safe to use this argument.]([http://gatkforums.broadinstitute.org/discussion/1252/unifiedgenotyper-gets-hung-waiting-for-file-lock)\n",
      "\n",
      "* ERROR MESSAGE: Bad input: Values for InbreedingCoeff annotation not detected for ANY training variant in the input callset. VariantAnnotator may be used to add these annotations.\n",
      "    * \u5982\u679c\u4f60\u7684\u6837\u54c1\u4e0a\u5c11\u4e8e10\u4e2a\uff0c\u8bf7\u53bb\u6389\u53c2\u6570 -an InbreedingCoeff\n",
      "    * \u5426\u5219\uff0c\u8bf7\u5c1d\u8bd5\u4f7f\u7528 VariantAnnotator\n",
      "        * http://gatkforums.broadinstitute.org/discussion/1406/error-values-for-inbreedingcoeff-annotation-not-detected-for-any-training-variant-in-the-input"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "SNP\u6ce8\u91ca"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Annovar\u6ce8\u91ca"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Annovar\u5b89\u88c5\u548c\u6570\u636e\u5e93\u4e0b\u8f7d"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "wget http://www.openbioinformatics.org/annovar/download/mP628pfL21/annovar.latest.tar.gz\n",
      "tar xvzf annovar.latest.tar.gz\n",
      "echo \"export \\${PATH}=\\${PATH}:`pwd`/annovar\" >>~/.bashsrc\n",
      "source ~/.bashrc\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```make\n",
      "cat <<END >Makefile\n",
      "#makefile\u8bed\u53e5\n",
      "annovar_dir=$(dir $(shell which annotate_variation.pl))\n",
      "\n",
      "download_annovar_db:\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            cytoBand $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            phastConsElements46way $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            1000g2012apr $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            genomicSuperDups $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar refGene $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar snp138 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar ljb23_all $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar esp6500si_all $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar cg46 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar cg69 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar cosmic68 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar cosmic68wgs $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar cadd $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar caddgt20 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar caddgt10 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar gerp++gt2 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar gerp++elem $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar ensGene $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar knownGene $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar popfreq_all $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar clinvar_20140702 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar nci60 $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar snp138NonFlagged $(annovar_dir)humandb/\n",
      "        annotate_variation.pl -downdb -buildver hg19 \\\n",
      "            -webfrom annovar 1000g2012apr $(annovar_dir)humandb/\n",
      "END\n",
      "make download_annovar_db\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Annovar\u6ce8\u91ca"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "#For hard-filter\n",
      "#Convert format\n",
      "convert2annovar.pl --format vcf4 Exosome.genotypeGVCF.SNPs.hardfilter.vcf \\\n",
      "    --outfile Exosome.genotypeGVCF.SNPs.hardfilter.vcf.ai \\\n",
      "    --allsample --includeinfo --comment --withzyg\n",
      "convert2annovar.pl --format vcf4 Exosome.genotypeGVCF.INDEL.hardfilter.vcf \\\n",
      "    --outfile Exosome.genotypeGVCF.INDEL.hardfilter.vcf.ai \\\n",
      "    --allsample --includeinfo --comment --withzyg\n",
      "#Begin annotation\n",
      "table_annovar.pl --remove --buildver hg19 \\\n",
      "    --outfile Exosome.genotypeGVCF.SNPs.hardfilter.vcf.avoutput --nastring . \\\n",
      "    --protocol refGene,cytoBand,genomicSuperDups,esp6500si_all,1000g2012apr_all,snp138,ljb23_all \\\n",
      "    --operation g,r,r,f,f,f,f \\\n",
      "    Exosome.genotypeGVCF.SNPs.hardfilter.vcf.ai.NA19129.avinput humandb/\n",
      "table_annovar.pl --remove --buildver hg19 \\\n",
      "    --outfile Exosome.genotypeGVCF.INDEL.hardfilter.vcf.avoutput --nastring . \\\n",
      "    --protocol refGene,cytoBand,genomicSuperDups,esp6500si_all,1000g2012apr_all,snp138,ljb23_all \\\n",
      "    --operation g,r,r,f,f,f,f \\\n",
      "    Exosome.genotypeGVCF.INDEL.hardfilter.vcf.ai.NA19129.avinput humandb/\n",
      "\n",
      "table_annovar.pl --remove --buildver hg19 \\\n",
      "    --outfile Exosome.genotypeGVCF.SNPs.hardfilter.vcf.avoutput --nastring . \\\n",
      "    --protocol refGene,cytoBand,genomicSuperDups,esp6500si_all,1000g2012apr_all,snp138,ljb23_all \\\n",
      "    --operation g,r,r,f,f,f,f \\\n",
      "    Exosome.genotypeGVCF.SNPs.hardfilter.vcf.ai.NA19240.avinput humandb/\n",
      "table_annovar.pl --remove --buildver hg19 \\\n",
      "    --outfile Exosome.genotypeGVCF.INDEL.hardfilter.vcf.avoutput --nastring . \\\n",
      "    --protocol refGene,cytoBand,genomicSuperDups,esp6500si_all,1000g2012apr_all,snp138,ljb23_all \\\n",
      "    --operation g,r,r,f,f,f,f \\\n",
      "    Exosome.genotypeGVCF.INDEL.hardfilter.vcf.ai.NA19240.avinput humandb/\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* --format vcf4\uff1a\u5236\u5b9a\u8f93\u5165\u683c\u5f0f\n",
      "* --allsample: \u8f6c\u6362vcf\u6587\u4ef6\u4e2d\u6240\u6709\u6837\u54c1\uff0c\u9ed8\u8ba4\u53ea\u8f6c\u6362\u7b2c\u4e00\u4e2a\u6837\u54c1\n",
      "* --includeinfo: \u5305\u542bvcf\u6587\u4ef6\u81ea\u5e26\u7684\u6ce8\u91ca\u4fe1\u606f\n",
      "* --comment: \u4fdd\u7559vcf\u7684\u6ce8\u91ca\u884c\n",
      "* --withzyg: \u540c\u65f6\u8f93\u51fazygosity/coverage/quality\n",
      "* --nastring: string to display when a score is not available\n",
      "* --protocol: comma-delimited string specifying database protocol \n",
      "    * `esp6500si_all` means allele frequency in the ESP6500 database. \n",
      "    * `snp138` means the SNP identifier in the dbSNP version 138. \n",
      "    * `LJB23*` contains prediction scores for non-synonymous variants using several widely used tools, including SIFT scores, PolyPhen2 HDIV scores, PolyPhen2 HVAR scores, LRT scores, MutationTaster scores, MutationAssessor score, FATHMM scores, GERP++ scores, PhyloP scores and SiPhy scores. \n",
      "* --operation: tells ANNOVAR which operations to use for each of the protocols: `g` means gene-based, `r` means region-based and `f` means filter-based."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For GATK: java -Xmx6g -jar GenomeAnalysisTK.jar -R example.fa -T UnifiedGenotper -I sampleSort.bam -o GATK.vcf -mbq 30 -glm BOTH\n",
      "\n",
      "For samtools: samtools mpileup -Q 13 -ugf example.fa sampleSort.bam | bcftools view -bvcg > sample.raw.bcf bcftools view sample.raw.bcf | vcfutils.pl varFilter -d 10 -w 5 -D 100 > samtools.vcf\n",
      "\n",
      "https://www.biostars.org/p/65413/"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}